{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_ap12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5SkwNOH_WHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "#########################################################################\n",
        "############ CARGAR LAS LIBRERÍAS NECESARIAS ############################\n",
        "#########################################################################\n",
        "\n",
        "# Importar librerı́as necesarias\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.utils as np_utils\n",
        "# Importar modelos y capas que se van a usar\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization\n",
        "# Importar el optimizador a usar\n",
        "from keras.optimizers import SGD\n",
        "# Importar el conjunto de datos\n",
        "from keras.datasets import cifar100\n",
        "\n",
        "# Import image proprocessors\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Import Early Stopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "# TensorFlow no muestra warnings\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "#########################################################################\n",
        "######## FUNCIÓN PARA CARGAR Y MODIFICAR EL CONJUNTO DE DATOS ###########\n",
        "#########################################################################\n",
        "\n",
        "\"\"\" Sólo se le llama una vez. Devuelve 4 vectores conteniendo, por este orden, las imágenes de entrenamiento,\n",
        "    las clases de las imagenes de entrenamiento, las imágenes del conjunto de test y las clases del conjunto de test.\n",
        "\"\"\"\n",
        "def cargarImagenes():\n",
        "    # Cargamos Cifar100. Cada imagen tiene tamaño (32 , 32 , 3). Nos quedamos con las imágenes de 25 de las clases.\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "\n",
        "    train_idx = np.isin(y_train, np.arange (25))\n",
        "    train_idx = np.reshape(train_idx, -1)\n",
        "    x_train = x_train[train_idx]\n",
        "    y_train = y_train[train_idx]\n",
        "\n",
        "    test_idx = np.isin(y_test, np.arange (25))\n",
        "    test_idx = np.reshape(test_idx, -1)\n",
        "    x_test = x_test[test_idx]\n",
        "    y_test = y_test[test_idx]\n",
        "\n",
        "    # Transformamos los vectores de clases en matrices. Cada componente se convierte en un vector de\n",
        "    # ceros con un uno en la componente correspondiente a la clase a la que pertenece la imagen.\n",
        "    y_train = np_utils.to_categorical(y_train, 25)\n",
        "    y_test = np_utils.to_categorical(y_test, 25)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "#########################################################################\n",
        "######## FUNCIÓN PARA OBTENER EL ACCURACY DEL CONJUNTO DE TEST ##########\n",
        "#########################################################################\n",
        "\n",
        "\"\"\" Devuelve el accuracy de un modelo (porcentaje de etiquetas bien predichas frente al total).\n",
        "- labels: vector de etiquetas verdaderas.\n",
        "- preds: vector de etiquetas predichas.\n",
        "\"\"\"\n",
        "def calcularAccuracy (labels, preds):\n",
        "    labels = np.argmax(labels, axis = 1)\n",
        "    preds = np.argmax(preds, axis = 1)\n",
        "\n",
        "    accuracy = sum (labels == preds)/ len(labels)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "#########################################################################\n",
        "## FUNCIÓN PARA PINTAR LA PÉRDIDA Y EL ACCURACY EN TRAIN Y VALIDACIÓN ###\n",
        "#########################################################################\n",
        "\n",
        "\n",
        "\"\"\" Pinta dos gráficas, una con la evolución de la función de pérdida en el conjunto de train y en el de validación,\n",
        "    y otra con la evolución del accuracy en el conjunto de train y el de validación.\n",
        "- hist: historial del entrenamiento del modelo (salida de fit () y fit_generator ()).\n",
        "\"\"\"\n",
        "def mostrarEvolucion(hist):\n",
        "    loss = hist.history ['loss']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.legend(['Training loss', 'Validation loss'])\n",
        "    plt.show()\n",
        "\n",
        "    acc = hist.history['acc']\n",
        "    val_acc = hist.history['val_acc']\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc )\n",
        "    plt.legend(['Training accuracy', 'Validation accuracy'])\n",
        "    plt.show()\n",
        "\n",
        "#########################################################################\n",
        "################## DEFINICIÓN DEL MODELO BASENET ########################\n",
        "#########################################################################\n",
        "\n",
        "\"\"\" Definición del modelo BaseNet del apartado 1.\n",
        "- input_shape: dimesión de la entrada\n",
        "\"\"\"\n",
        "def model_baseNet(input_shape=(32,32,3)):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(6, kernel_size=(5,5), activation='relu', input_shape=input_shape))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Conv2D(16, kernel_size=(5,5), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(25, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "#########################################################################\n",
        "######### DEFINICIÓN DEL OPTIMIZADOR Y COMPILACIÓN DEL MODELO ###########\n",
        "#########################################################################\n",
        "\n",
        "\"\"\" Definimos el optimizador y compilador del modelo.\n",
        "- model: el modelo.\n",
        "\"\"\"\n",
        "def optimizadorCompilador(model):\n",
        "  # Definimos la función pérdida a minimizar.Clasificación multiclase -> categorical_crossentropy\n",
        "  # Argumento metrics las métricas que se quieren calcular a lo largo de todas las épocas.\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "            optimizer=SGD (lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True),\n",
        "            metrics=['accuracy'])\n",
        "  \n",
        "  # Usar uno de los siguientes optimizadores:\n",
        "  # optimizer=optimizer=keras.optimizers.Adadelta()\n",
        "  # optimizer=SGD (lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
        "  \n",
        "  # Teniendo el modelo base guardamos los pesos aleatorios con los que empieza la red, para poder\n",
        "  # reestablecerlos después y comparar resultados entre no usar mejoras y sí usarlas.\n",
        "  weights = model.get_weights()\n",
        "  return weights\n",
        "\n",
        "#########################################################################\n",
        "###################### ENTRENAMIENTO DEL MODELO #########################\n",
        "#########################################################################\n",
        "\n",
        "\"\"\" Definimos el entrenamiento del modelo.\n",
        "- model: el modelo.\n",
        "- x_train: datos de entrenamiento.\n",
        "- y_train: etiquetas de los datos de entrenamiento.\n",
        "- datagen: nuestro ImageDataGenerator.\n",
        "- batch_size: \n",
        "- ephocs: épocas.\n",
        "- verbose: nos muestra información al hacer el ajuste.\n",
        "\"\"\"\n",
        "def train(model, x_train, y_train, datagen, batch_size=64, epochs=20, verbose=0):\n",
        "  # Entrenamos el modelo con fit que recibe las imágenes de entrenamiento directamente.\n",
        "  # histograma = model.fit(x_train, y_train,\n",
        "  #      batch_size=batch_size,\n",
        "  #      epochs=epochs,\n",
        "  #      verbose=verbose,\n",
        "  #      validation_data=(x_test, y_test))\n",
        "  \n",
        "  train_data = datagen.flow(x_train, y_train, batch_size=batch_size, subset='training')\n",
        "  validation_data = datagen.flow(x_train, y_train, batch_size=batch_size, subset='validation')\n",
        "\n",
        "  hist = model.fit_generator(train_data,\n",
        "             steps_per_epoch = len(x_train)*0.9/batch_size,\n",
        "             epochs = epochs,\n",
        "             verbose = verbose,\n",
        "             validation_data = validation_data,\n",
        "             validation_steps = len (x_train)*0.1/batch_size)\n",
        "\n",
        "  return hist\n",
        "\n",
        "#########################################################################\n",
        "################ PREDICCIÓN SOBRE EL CONJUNTO DE TEST ###################\n",
        "#########################################################################\n",
        "\n",
        "\"\"\" Predicción sobre el conjunto de test.\n",
        "- model: el modelo usado.\n",
        "- x_test: conjunto de test.\n",
        "- y_test: etiquetas del conjunto de test.\n",
        "\"\"\"\n",
        "def prediccion(model, x_test, y_test):\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "\"\"\" Ejercicio1 \"\"\"\n",
        "def ejercicio1():\n",
        "  input_shape=(32, 32, 3)     # Imágenes en color con 3 canales de 32x32 píxeles.\n",
        "  num_classes = 25            # Número de clases es 25\n",
        "  batch_size = 64             # Tamaño de batch potencia de 2\n",
        "  epochs = 20                 # Elegimos número de épocas\n",
        "\n",
        "  x_train, y_train, x_test, y_test = cargarImagenes()\n",
        "  datagen = ImageDataGenerator(validation_split = 0.1)\n",
        "  datagen.fit(x_train)\n",
        "\n",
        "  model = model_baseNet(input_shape)\n",
        "  model.summary()             # Descripción del modelo\n",
        "  weights = optimizadorCompilador(model)\n",
        "  model.set_weights(weights)  # Reestablecemos los pesos\n",
        "\n",
        "  hist = train(model, x_train, y_train, datagen, batch_size, epochs, verbose=1)\n",
        "  print(hist)\n",
        "  mostrarEvolucion(hist)\n",
        "  prediccion(model, x_test, y_test)\n",
        "\n",
        "ejercicio1()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xK4pNEVOTBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########################################################################\n",
        "########################## MEJORA DEL MODELO ############################\n",
        "#########################################################################\n",
        "\n",
        "# NORMALIZACIÓN. Utilice la clase ImageDataGenerator con los parámetros correctos para que los datos estén bien\n",
        "# condicionados (media=0, std dev=1) para mejorar el entrenamiento.\n",
        "\n",
        "\"\"\" Ejercicio2 - NORMALIZACIÓN \"\"\"\n",
        "def ejercicio2_normalizacion():\n",
        "  print(\"NORMALIZACIÓN\")\n",
        "  input_shape=(32, 32, 3)     # Imágenes en color con 3 canales de 32x32 píxeles.\n",
        "  num_classes = 25            # Número de clases es 25\n",
        "  batch_size = 32             # Tamaño de batch potencia de 2\n",
        "  epochs = 20                 # Elegimos número de épocas\n",
        "\n",
        "  x_train, y_train, x_test, y_test = cargarImagenes()\n",
        "  datagen_train = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, validation_split=0.1)\n",
        "  datagen_test = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "  datagen_train.fit(x_train)\n",
        "  datagen_test.fit(x_train)\n",
        "\n",
        "  model = model_baseNet(input_shape)\n",
        "  model.summary()             # Descripción del modelo\n",
        "  weights = optimizadorCompilador(model)\n",
        "  model.set_weights(weights)  # Reestablecemos los pesos\n",
        "\n",
        "  hist = train(model, x_train, y_train, datagen_train, batch_size, epochs, verbose=1)\n",
        "  print(hist)\n",
        "  mostrarEvolucion(hist)\n",
        "  preds = model.predict_generator(datagen_test.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n",
        "  score = calcularAccuracy(y_test, preds)\n",
        "  print('Test accuracy:', score)\n",
        "\n",
        "ejercicio2_normalizacion()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmS_6FeLfTn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUMENTO DE DATOS. Usar parámetros de aumento de datos de la clase ImageDataGenerator, como zoom_range\n",
        "# y/o horizontal_flip. No debería tener ningún aumento de datos en los conjuntos de validación ni test.\n",
        "\n",
        "\"\"\" Ejercicio2 - AUMENTO DE DATOS. \"\"\"\n",
        "def ejercicio2_aumento():\n",
        "  print(\"AUMENTO DE DATOS\")\n",
        "  input_shape=(32, 32, 3)     # Imágenes en color con 3 canales de 32x32 píxeles.\n",
        "  num_classes = 25            # Número de clases es 25\n",
        "  batch_size = 64             # Tamaño de batch potencia de 2\n",
        "  epochs = 20                 # Elegimos número de épocas\n",
        "\n",
        "  x_train, y_train, x_test, y_test = cargarImagenes()\n",
        "  datagen_train = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, horizontal_flip=True, zoom_range=0.7, validation_split=0.1)\n",
        "  datagen_test = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "  datagen_train.fit(x_train)\n",
        "  datagen_test.fit(x_train)\n",
        "\n",
        "  model = model_baseNet(input_shape)\n",
        "  model.summary()             # Descripción del modelo\n",
        "  weights = optimizadorCompilador(model)\n",
        "  model.set_weights(weights)  # Reestablecemos los pesos\n",
        "\n",
        "  hist = train(model, x_train, y_train, datagen_train, batch_size, epochs, verbose=1)\n",
        "  print(hist)\n",
        "  mostrarEvolucion(hist)\n",
        "  preds = model.predict_generator(datagen_test.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n",
        "  score = calcularAccuracy(y_test, preds)\n",
        "  print('Test accuracy:', score)\n",
        "\n",
        "ejercicio2_aumento()\n",
        "\n",
        "\"\"\" Ejercicio2_v2 - AUMENTO DE DATOS. \"\"\"\n",
        "def ejercicio2_aumento_v2():\n",
        "  print(\"AUMENTO DE DATOS - V2\")\n",
        "  input_shape=(32, 32, 3)     # Imágenes en color con 3 canales de 32x32 píxeles.\n",
        "  num_classes = 25            # Número de clases es 25\n",
        "  batch_size = 32             # Tamaño de batch potencia de 2\n",
        "  epochs = 20                 # Elegimos número de épocas\n",
        "\n",
        "  x_train, y_train, x_test, y_test = cargarImagenes()\n",
        "  datagen_train = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True,\n",
        "                                     vertical_flip=True, zoom_range=0.7, validation_split=0.1)\n",
        "  datagen_test = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "  datagen_train.fit(x_train)\n",
        "  datagen_test.fit(x_train)\n",
        "\n",
        "  model = model_baseNet(input_shape)\n",
        "  model.summary()             # Descripción del modelo\n",
        "  weights = optimizadorCompilador(model)\n",
        "  model.set_weights(weights)  # Reestablecemos los pesos\n",
        "\n",
        "  hist = train(model, x_train, y_train, datagen_train, batch_size, epochs, verbose=1)\n",
        "  print(hist)\n",
        "  mostrarEvolucion(hist)\n",
        "  preds = model.predict_generator(datagen_test.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n",
        "  score = calcularAccuracy(y_test, preds)\n",
        "  print('Test accuracy:', score)\n",
        "\n",
        "#ejercicio2_aumento_v2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_v4gK0wfQGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RED MÁS PROFUNDA. Experimente agregando más capas convolucionales y totalmente conectadas.\n",
        "# Agregue más capas conv con canales de salida crecientes y también agregue más capas lineales (fc).\n",
        "# No coloque una capa de maxpool después de cada capa conv ya que conduce a una pérdida excesiva de información.\n",
        "\n",
        "\"\"\" Modelo baseNet primera mejora. \"\"\"\n",
        "def model_baseNet_mejora1(input_shape=(32,32,3)):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(8, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(16, kernel_size=(5, 5), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(75, activation='relu'))\n",
        "  model.add(Dense(25, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "def model_baseNet_mejora2(input_shape=(32,32,3)):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, kernel_size=(5,5), activation='relu', input_shape=input_shape))\n",
        "  #model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(4,4)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(8, kernel_size=(5,5), activation='relu'))\n",
        "  #model.add(BatchNormalization())\n",
        "  model.add(Conv2D(4, kernel_size=(3,3), activation='relu', padding = 'same'))\n",
        "  #model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(25, activation='relu'))\n",
        "  model.add(Dense(25, activation='softmax'))\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\"\"\" Ejercicio2 - RED MÁS PROFUNDA. \"\"\"\n",
        "def ejercicio2_red():\n",
        "  print(\"RED MÁS PROFUNDA\")\n",
        "  input_shape=(32, 32, 3)     # Imágenes en color con 3 canales de 32x32 píxeles.\n",
        "  num_classes = 25            # Número de clases es 25\n",
        "  batch_size = 32             # Tamaño de batch potencia de 2\n",
        "  epochs = 20                 # Elegimos número de épocas\n",
        "\n",
        "  x_train, y_train, x_test, y_test = cargarImagenes()\n",
        "  datagen_train = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, horizontal_flip=True, zoom_range=0.7, validation_split=0.1)\n",
        "  datagen_test = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "  datagen_train.fit(x_train)\n",
        "  datagen_test.fit(x_train)\n",
        "\n",
        "  model = model_baseNet_mejora1(input_shape)\n",
        "  #model = model_baseNet_mejora2(input_shape)\n",
        "  model.summary()             # Descripción del modelo\n",
        "  weights = optimizadorCompilador(model)\n",
        "  model.set_weights(weights)  # Reestablecemos los pesos\n",
        "\n",
        "  hist = train(model, x_train, y_train, datagen_train, batch_size, epochs, verbose=1)\n",
        "  print(hist)\n",
        "  mostrarEvolucion(hist)\n",
        "  preds = model.predict_generator(datagen_test.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n",
        "  score = calcularAccuracy(y_test, preds)\n",
        "  print('Test accuracy:', score)\n",
        "\n",
        "ejercicio2_red()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lFVDMfnfLCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CAPAS DE NORMALIZACIÓN. Las capas de normalización ayudan a reducir el sobreajuste y mejorar el entrenamiento\n",
        "# del modelo. Las capas de normalización de Keras son una forma fácil de incorporarlas al modelo. Agregue capas de\n",
        "# normalización después de las capas conv (BatchNormalization). Agregue capas de normalización después de capas lineales\n",
        "# y experimente insertándolas antes o después de las capas ReLU.\n",
        "\n",
        "\"\"\" Modelo baseNet primera mejora (BatchNormalization()). \"\"\"\n",
        "def model_baseNet_batch(input_shape=(32,32,3)):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', input_shape=input_shape))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(16, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(6, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(25, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "\"\"\" Ejercicio2 - CAPAS DE NORMALIZACIÓN. \"\"\"\n",
        "def ejercicio2_capa_normalizacion():\n",
        "  print(\"CAPAS DE NORMALIZACIÓN\")\n",
        "  input_shape=(32, 32, 3)     # Imágenes en color con 3 canales de 32x32 píxeles.\n",
        "  num_classes = 32            # Número de clases es 25\n",
        "  batch_size = 64             # Tamaño de batch potencia de 2\n",
        "  epochs = 20                 # Elegimos número de épocas\n",
        "\n",
        "  x_train, y_train, x_test, y_test = cargarImagenes()\n",
        "  datagen_train = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, horizontal_flip=True, zoom_range=0.7, validation_split=0.1)\n",
        "  datagen_test = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "  datagen_train.fit(x_train)\n",
        "  datagen_test.fit(x_train)\n",
        "\n",
        "  model = model_baseNet_batch(input_shape)\n",
        "  model.summary()             # Descripción del modelo\n",
        "  weights = optimizadorCompilador(model)\n",
        "  model.set_weights(weights)  # Reestablecemos los pesos\n",
        "\n",
        "  hist = train(model, x_train, y_train, datagen_train, batch_size, epochs, verbose=1)\n",
        "  print(hist)\n",
        "  mostrarEvolucion(hist)\n",
        "  preds = model.predict_generator(datagen_test.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n",
        "  score = calcularAccuracy(y_test, preds)\n",
        "  print('Test accuracy:', score)\n",
        "\n",
        "ejercicio2_capa_normalizacion()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7FJn6IMe7WM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"EARLY STOPPING\". ¿Después de cuántas épocas parar y dejar de entrenar?\n",
        "\n",
        "\"\"\" Ejercicio2 - EARLY STOPPING \"\"\"\n",
        "def ejercicio2_early_stopping():\n",
        "  print(\"EARLY STOPPING\")\n",
        "  input_shape=(32, 32, 3)     # Imágenes en color con 3 canales de 32x32 píxeles.\n",
        "  num_classes = 25            # Número de clases es 25\n",
        "  batch_size = 32             # Tamaño de batch potencia de 2\n",
        "  epochs = 20                 # Elegimos número de épocas\n",
        "\n",
        "  x_train, y_train, x_test, y_test = cargarImagenes()\n",
        "  datagen_train = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, horizontal_flip=True, zoom_range=0.7, validation_split=0.1)\n",
        "  datagen_test = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "  datagen_train.fit(x_train)\n",
        "  datagen_test.fit(x_train)\n",
        "\n",
        "  model = model_baseNet_batch(input_shape)\n",
        "  model.summary()             # Descripción del modelo\n",
        "  weights = optimizadorCompilador(model)\n",
        "  model.set_weights(weights)  # Reestablecemos los pesos\n",
        "\n",
        "  train_data = datagen_train.flow(x_train, y_train, batch_size=batch_size, subset='training')\n",
        "  validation_data = datagen_train.flow(x_train, y_train, batch_size=batch_size, subset='validation')\n",
        "  es = EarlyStopping(monitor='val_loss', patience=5)\n",
        "  hist = model.fit_generator(train_data,\n",
        "            steps_per_epoch = len(x_train)*0.9/batch_size,\n",
        "            epochs = epochs,\n",
        "            verbose = 1,\n",
        "            validation_data = validation_data,\n",
        "            validation_steps = len (x_train)*0.1/batch_size,\n",
        "            callbacks=[es])\n",
        "  print(hist)\n",
        "  mostrarEvolucion(hist)\n",
        "  preds = model.predict_generator(datagen_test.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n",
        "  score = calcularAccuracy(y_test, preds)\n",
        "  print('Test accuracy:', score)\n",
        "\n",
        "ejercicio2_early_stopping()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}